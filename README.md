# ğŸ•¸ï¸ AI Web Scraper

An intelligent web scraping tool powered by **LLMs** (Locally hosted via Ollama) to extract meaningful and structured information from websites using natural language prompts.

---

## ğŸš€ Tech Stack

- [Streamlit](https://streamlit.io/) â€” Web frontend for user interaction  
- [LangChain](https://www.langchain.com/) â€” LLM orchestration  
- [langchain_ollama](https://python.langchain.com/docs/integrations/llms/ollama) â€” LLM integration via Ollama  
- [Selenium](https://www.selenium.dev/) â€” Web automation and scraping  
- [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/) â€” DOM parsing  
- `lxml` and `html5lib` â€” HTML parsers  
- `python-dotenv` â€” Environment variable management

---

## ğŸ§  Why AI Web Scraping?

Traditional web scraping is fragile â€” HTML structure changes often, making hardcoded scrapers break easily. This project solves that by:

- Letting you **describe what you want in natural language**
- Parsing **complex or dynamic sites** using a real browser (Selenium)
- Using **LLMs (like LLaMA 3)** locally to extract content based on your prompt
- Handling **captcha/IP blocks** with optional **Bright Data's Scraping Browser**

This enables **flexible, adaptable scraping** even on JavaScript-heavy or protected websites.

---

## âš™ï¸ How It Works

1. User enters a URL into the **Streamlit UI**
2. The backend uses **Selenium + chromedriver** to scrape the siteâ€™s content
3. If blocked or facing captchas, **Bright Data's Scraping Browser** can be used
4. The raw HTML is cleaned with **BeautifulSoup**
5. The HTML is chunked and passed to a **local LLM via LangChain/Ollama**
6. The model extracts exactly whatâ€™s requested via prompt

---

## ğŸ“¦ Installation & Setup

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/ai-web-scraper.git
cd ai-web-scraper
```
## 2 .Install Python Dependencies
Create a virtual environment (optional but recommended):

```bash
python -m venv venv
source venv/bin/activate  # or venv\Scripts\activate on Windows
```
## Install dependencies:
```bash
pip install -r requirements.txt
```

## 3. Install & Set Up Ollama
Download and install Ollama from: https://ollama.com
Then pull the LLaMA model (or another supported one):
```bash
ollama pull llama3
```

## 4. Add Environment Variables (Optional)
If using Bright Data, create a .env file:
```bash
SBR_WEBDRIVER=https://your-brightdata-webdriver-endpoint
```
## See their docs: https://brightdata.com/products/scraping-browser
## 5.  Run the Application
```bash
streamlit run app.py
```
## ğŸ’¡ Features
# âœ… Works with both static & dynamic pages
# âœ… Robust against structural changes in websites
# âœ… Language-model-powered parsing
# âœ… Optional captcha/IP bypass with Bright Data
# âœ… Fully local â€” no API key needed if using Ollama

## ğŸ” Handling Protected Sites
# If a website has strong anti-bot protections (e.g., captchas, IP bans), enable the Bright Data method (see commented code in scraper.py) and configure your credentials.

## ğŸ“ Project Structure
```bash
ai-web-scraper/
â”œâ”€â”€ app.py                # Streamlit frontend
â”œâ”€â”€ scraper.py            # Web scraping and content extraction
â”œâ”€â”€ parser.py             # LangChain + Ollama-based parser
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## âœ¨ Use Cases
# Market intelligence
# Data aggregation
# Real-time content extraction
# News/article parsing with LLM
# Research automation

# ğŸ“Œ Notes
# Make sure chromedriver is in the project directory or PATH.
# Ollama must be running in the background for the LLM to work.
# Avoid overloading the model with huge pages â€” DOM chunking is built-in.
